threats:
  - id: "ML-001"
    name: "Training Data Poisoning"
    category: "Data Integrity"
    stride_category: "Tampering"
    likelihood: "Medium"
    impact: "High"
    risk_score: 8
    description: "Attackers inject malicious data into training datasets"
    business_impact: "$10M+ annual fraud losses"
    attack_scenarios:
      - "Gradual injection of fraudulent transactions labeled as legitimate"
      - "Coordinated account creation with poisoned transaction history"
      - "Compromise of external data sources used for training"
    detection_methods:
      - "Statistical drift detection in training data"
      - "Data lineage and provenance tracking"
      - "Anomaly detection in feature distributions"
    current_controls: "None"
    recommended_controls:
      - "Robust data validation pipelines"
      - "Human-in-the-loop verification for training data"
      - "Adversarial training techniques"
    estimated_cost: "$500K"

  - id: "ML-002"
    name: "Adversarial Evasion"
    category: "Model Manipulation"
    stride_category: "Spoofing"
    likelihood: "High"
    impact: "High"
    risk_score: 9
    description: "Crafted inputs designed to fool the ML model into misclassification"
    business_impact: "$5M+ annual losses from bypassed fraud detection"
    attack_scenarios:
      - "Carefully crafted transaction amounts and patterns"
      - "Timing-based attacks to exploit model weaknesses"
      - "Feature manipulation to stay below detection thresholds"
    detection_methods:
      - "Adversarial input detection algorithms"
      - "Ensemble model voting discrepancies"
      - "Real-time anomaly detection in prediction confidence"
    current_controls: "Basic input validation"
    recommended_controls:
      - "Adversarial training and robust optimization"
      - "Input preprocessing and feature normalization"
      - "Multiple model consensus mechanisms"
    estimated_cost: "$300K"